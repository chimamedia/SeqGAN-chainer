{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import optimizers\n",
    "from chainer import cuda\n",
    "from chainer import serializers\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "\n",
    "sys.path.append('../../chainer-SeqGAN/')\n",
    "\n",
    "from models import SeqGAN, TextCNN, SeqEncoder\n",
    "from optimizer_hook import NamedWeightDecay\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--out\", default='')\n",
    "parser.add_argument(\"--gen\", default='')\n",
    "parser.add_argument(\"--dis\", default='')\n",
    "parser.add_argument('--gpu', '-g', type=int, default=0)\n",
    "parser.add_argument('--parallel', '-p', default=0, type=int)\n",
    "\n",
    "#  Generator  Hyper-parameters\n",
    "parser.add_argument(\"--gen_emb_dim\", type=int, default=128)\n",
    "parser.add_argument(\"--gen_hidden_dim\", type=int, default=128)\n",
    "parser.add_argument(\"--gen_grad_clip\", type=int, default=5)\n",
    "parser.add_argument(\"--gen_lr\", type=float, default=1e-3)\n",
    "parser.add_argument(\"--num_lstm_layer\", type=int, default=1)\n",
    "parser.add_argument(\"--no-dropout\", dest='dropout', action='store_false', default=True)\n",
    "\n",
    "#  Discriminator  Hyper-parameters\n",
    "parser.add_argument(\"--dis_embedding_dim\", type=int, default=64)\n",
    "parser.add_argument(\"--dis_filter_sizes\", default=\"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20\")\n",
    "parser.add_argument(\"--dis_num_filters\", default=\"100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160\")\n",
    "parser.add_argument(\"--dis_dropout_keep_prob\", type=float, default=0.75)\n",
    "parser.add_argument(\"--dis_l2_reg_lambda\", type=float, default=0.2)\n",
    "parser.add_argument(\"--dis_lr\", type=float, default=1e-4)\n",
    "\n",
    "#  Training  Hyper-parameters\n",
    "parser.add_argument(\"--batch_size\", type=int, default=100)\n",
    "parser.add_argument(\"--total_epoch\", type=int, default=800)\n",
    "parser.add_argument(\"--gen_pretrain_epoch\", type=int, default=100)\n",
    "parser.add_argument(\"--dis_pretrain_epoch\", type=int, default=50)\n",
    "\n",
    "parser.add_argument(\"--g_steps\", type=int, default=1)\n",
    "parser.add_argument(\"--d_steps\", type=int, default=5)\n",
    "parser.add_argument(\"--K\", type=int, default=5)\n",
    "\n",
    "parser.add_argument(\"--rollout_update_ratio\", type=float, default=0.8)\n",
    "parser.add_argument(\"--sample_per_iter\", type=int, default=10000)\n",
    "\n",
    "parser.add_argument(\"--free-pretrain\", dest='free_pretrain', action='store_true', default=False)\n",
    "parser.add_argument(\"--ae-pretrain\", dest='ae_pretrain', action='store_true', default=False)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nico-comment dataset loader\n",
    "with open('nico_comment_processed.dat', 'rb') as f:\n",
    "    train_comment_data, test_comment_data, train_tag_data, test_tag_data, vocab, tag_id = pickle.load(f)\n",
    "\n",
    "train_num = len(train_comment_data)\n",
    "test_num = len(test_comment_data)\n",
    "vocab_size = 3000\n",
    "seq_length = 30\n",
    "start_token = 0\n",
    "\n",
    "def progress_report(count, start_time, batchsize):\n",
    "    duration = time.time() - start_time\n",
    "    throughput = count * batchsize / duration\n",
    "    sys.stderr.write(\n",
    "        '\\r{} updates ({} samples) time: {} ({:.2f} samples/sec)'.format(\n",
    "            count, count * batchsize, str(datetime.timedelta(seconds=duration)).split('.')[0], throughput\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator = SeqGAN(vocab_size=vocab_size, emb_dim=args.gen_emb_dim, hidden_dim=args.gen_hidden_dim,\n",
    "                   sequence_length=seq_length, start_token=start_token, lstm_layer=1,\n",
    "                   dropout=args.dropout, free_pretrain=args.free_pretrain, encoder=None).to_gpu()\n",
    "\n",
    "serializers.load_hdf5('runs/first/models/gen_pretrain_20.model', generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-90298ec2a243>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m discriminator = TextCNN(num_classes=2, vocab_size=vocab_size, embedding_size=args.dis_embedding_dim,\n\u001b[0m\u001b[0;32m      2\u001b[0m                         \u001b[0mfilter_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdis_filter_sizes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                         \u001b[0mnum_filters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdis_num_filters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                         ).to_gpu()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TextCNN' is not defined"
     ]
    }
   ],
   "source": [
    "discriminator = TextCNN(num_classes=2, vocab_size=vocab_size, embedding_size=args.dis_embedding_dim,\n",
    "                        filter_sizes=[int(n) for n in args.dis_filter_sizes.split(',')],\n",
    "                        num_filters=[int(n) for n in args.dis_num_filters.split(',')]\n",
    "                        ).to_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pre-trained test_loss 1.4349755005041758\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "batch_size = 100\n",
    "perm = np.random.permutation(test_num)\n",
    "for i in range(0, test_num, batch_size):\n",
    "    batch = test_comment_data[perm[i:i + batch_size]]\n",
    "    g_loss = generator.pretrain_step(batch)\n",
    "    test_loss.append(float(g_loss.data))\n",
    "print('\\npre-trained test_loss {}'.format(np.mean(test_loss)))\n",
    "test_count = args.gen_pretrain_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive = train_comment_data[np.random.permutation(train_num)[:args.sample_per_iter]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "%prun negative = generator.generate(args.sample_per_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 30)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2148,  928,  362, 1158,  359,  358,  351,  347,  389, 1340, 1751,\n",
       "        358,  331,  358,  391,  326,    1, 2999, 2999, 2999, 2999, 2999,\n",
       "       2999, 2999, 2999, 2999, 2999, 2999, 2999, 2999], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
